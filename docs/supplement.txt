Coverage Gaps and Prompt Improvements for AI Code Scanner
Missing Prompt Categories
Access Control & Authorization Checks: No prompt currently checks for missing permission/role validations (e.g. unprotected admin actions or direct object reference issues). Including an Access Control category would cover OWASP’s “Broken Access Control” (high-priority for security).
Logging Practices: The prompts don’t address logging issues. We should add a check for insecure logging, e.g. code that logs sensitive data (passwords, PII) or fails to log important security events. This aligns with the project plan’s Logging and Error Handling focus and helps catch sensitive data exposure.
Authentication Logic Flaws: Beyond hardcoded secrets, there’s no prompt for weak authentication flows (e.g. using plaintext passwords or skipping token validation). A prompt could flag insecure password storage (like hashing with MD5 or comparing raw passwords) or missing verification steps. (This is medium priority – valuable but may be complex to implement in 8 hours.)
Input Validation Gaps: While we cover specific injections, there isn’t a general input validation prompt. For example, detecting functions that accept user input without any checks (which could lead to XSS or logic issues) could be a category. This is lower priority due to difficulty of inferring “missing” validation, but worth noting for future improvement.
Additional SQL Security: We have a SQL injection prompt, but other database security issues (like overly broad privileges or unsafe stored procedures) are out of scope. These can be deferred – focus on SQL injection as the primary SQL security category for the hackathon.
Prompt Structure & Output Format Improvements
Consistent Output Format: Currently, some prompts expect Markdown bullet lists (most Python checks) while others use JSON arrays (many SQL performance checks). For consistency and easier parsing, choose one format. Recommendation: Use markdown bullet lists for all findings during the hackathon (simpler to implement in the UI), or uniformly use a JSON structure for machine parsing. Consistency will reduce parsing logic complexity and risk of errors.
Uniform Template Style: Ensure each prompt template follows a similar tone and structure. (One template says “You are a security analyst…” while others use direct instructions.) Use a consistent instructive style (e.g. “Analyze the following code for … and report issues”) across all prompts. This uniformity helps the LLM follow expectations without confusion.
Category Naming Consistency: Standardize naming conventions for categories/subcategories. For example, avoid slashes in category names (currently “Maintainability/Anti-patterns”). We can split that into a clear category (“Maintainability”) and treat anti-pattern types as subcategories. Consistent naming will simplify how results are categorized and displayed.
Structured Issue Details: Every issue output should include a brief issue description, explanation, and recommendation. Most prompt templates already ask for this, but we should verify each one explicitly does. If using JSON, define keys like "issue", "explanation", "recommendation" uniformly. If using markdown, each bullet can follow a format (e.g. Issue: Description – Explanation. Fix: Recommendation). This consistency ensures the output is easy to read or parse.
Handle “No Issues” Case: Add a note in each prompt that if no problems are found, the model should state that explicitly (e.g. “No issues identified”). This prevents the LLM from feeling forced to invent an issue and makes the output clear when code is clean. It’s a small tweak to prompt wording that improves accuracy of results.
LLM Prompt Logic Enhancements (Accuracy & Efficiency)
Include Contextual Clues: Incorporate file or function context in the prompt to improve accuracy. For example, prepend a comment with the filename or function name (e.g. “File: auth.py, Function: delete_user”) before the code. This can cue the model about the code’s purpose (helpful for detecting auth or access issues) without a lengthy explanation. It’s optional, but if trivial to add, it can boost relevant issue identification.
Token Efficiency – Small Code Chunks: Ensure we feed the Llama model reasonably sized code snippets. For large files, break code into functions or logical blocks and scan them separately. This aligns with the “multi-round” approach and avoids prompt+code inputs that are too long (which could slow down inference or get truncated). It’s an efficiency measure critical for real-time scanning.
Combine Low-Risk Checks if Needed: To save time, consider merging some related prompts for a single pass. For instance, some Maintainability checks (naming conventions, comments, magic numbers) could be combined into one prompt run, since missing one of these is less critical than missing a security flaw. This reduces the number of model calls. (Only do this if needed for performance – the primary security and performance checks should remain separate to stay focused.)
Explicit Prompt Focus: Each prompt already targets a specific issue – continue to keep them narrowly focused. This specialization helps Llama 4 yield precise answers quickly. Avoid overloading any single prompt with too many goals. In practice, our current categories (SQL injection, OS command injection, etc.) are appropriately granular. Just ensure the language in each prompt only asks for one category of issue at a time, which improves the model’s accuracy per run.
Confidence or Severity Ratings (Future Option): If time permits, implement a follow-up question or output field for each finding indicating confidence or severity. For example, after listing issues, the model could tag each with a severity level (High/Med/Low) or a confidence score. This can help filter out likely false positives and prioritize issues in the report. This is an enhancement for accuracy (reducing noise), but it’s optional – only tackle it if core features are done early.
Skip Irrelevant Prompts: Optimize the scanning loop by detecting when certain category checks are unnecessary for a given snippet. For example, if a Python file has no SQL usage, we can skip the SQL injection prompt to save time. Simple heuristics (like searching the code for SQL keywords or dangerous function names) can gate whether to run a prompt. This pre-filtering avoids wasting inference calls and speeds up the overall scan without reducing coverage.
Hackathon Day Priorities (8-Hour Focus)
Core Security Checks First: Ensure the highest-impact security prompts are working and tested early. Prioritize SQL Injection, OS Command Injection, Hardcoded Secrets, Dangerous functions (eval, pickle), Path Traversal, Weak Cryptography, and Broad Exception handling. These align closely with OWASP Top 10 and will showcase the scanner’s value.
Key Performance Highlights: Implement one or two clear performance checks that are easy to demo. For example, catching SELECT * usage in SQL and nested loops (O(n^2)) in Python can visibly show performance analysis. These are straightforward patterns likely to appear in sample code. More nuanced performance tips (like join order or non-SARGable conditions) are valuable but ensure at least the obvious ones are covered in the demo.
Defer Minor Quality Checks: Nice-to-have prompts in Logical Correctness (off-by-one errors, misused API calls) and some Maintainability items (inconsistent naming, excessive parameters) can be lower priority. If time is running short, it’s better to deliver a solid security/performance scanner than to partially implement every single prompt. Focus on the categories that will impress in a short demo (security issues, major inefficiencies). The framework allows adding these extra checks later.
Test and Tune Output Format Early: Decide on markdown vs JSON output at the start and adjust prompts accordingly. Then do a quick end-to-end test with a small code snippet to make sure the LLM’s response format matches what the parsing/display logic expects. This will catch any formatting inconsistencies before they become a demo-time headache. For the hackathon timeframe, simplicity wins – a consistent bullet list output might be easiest to display directly in a UI.
Integration Over Expansion: Use the prepared prompts and get the end-to-end pipeline (code → prompt → Llama → result → display) running before adding new categories. It’s tempting to add (for example) the Access Control check right now, but if it risks breaking flow, it can be logged as a future enhancement. In an 8-hour sprint, it’s critical to have a working scanner tool, even if not exhaustive, rather than an almost-complete one that isn’t fully integrated.
Future Extensibility Notes
Multi-Language Support: The prompt design (language, category, subcategory structure) is generic and can accommodate other languages like JavaScript, Java, or C# in the future. We can extend the JSON catalog by adding entries for new languages (e.g. Java-specific SQL injection or buffer overflow in C). The scanning engine can loop through languages similarly, making the approach scalable beyond Python/SQL.
Additional Vulnerability Classes: The framework is ready to include other issue categories as needed. For instance, if expanding to front-end code, we could introduce prompts for things like XSS or insecure CORS configuration. The current separation of security/performance/correctness prompts would allow plugging these in without altering the core system. For now, we’ll keep the scope to Python and SQL, but this design decision will make future enhancement straightforward once the hackathon prototype is stable.